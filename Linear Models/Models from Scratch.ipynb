{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We are going to implement from scratch, with some help from\n",
    "numpy, a Perceptron, ADALINE, and Logistic Regression Model.\n",
    "We will be using the iris dataset to train and test our models.\n",
    "\"\"\"\n",
    "from math import exp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Before we get into defining our models, we will explore the iris dataset and \n",
    "figure out what features we should use for classification\n",
    "\"\"\"\n",
    "iris = pd.read_csv('iris.data', header=None, \n",
    "                   names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "\n",
    "## Let's see if we have linear separability with sepal length and petal length\n",
    "plt.scatter(iris.loc[0:49, 'sepal_length'], iris.loc[0:49, 'petal_length'], c='r', label=\"Iris-setosa\")\n",
    "plt.scatter(iris.loc[50:99, 'sepal_length'], iris.loc[50:99, 'petal_length'], c='b', label=\"Iris-versicolor\")\n",
    "plt.scatter(iris.loc[100:, 'sepal_length'], iris.loc[100:, 'petal_length'], c='g', marker='x', label=\"Iris-virginica\")\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('petal length (cm)')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From the scatter plot shown above, we can see that there is clear linear separability\n",
    "between iris-setosa and either of the other two species, however the line is blurred between \n",
    "iris-versicolor and iris-virginica. Our from-scratch models will use binary classification to \n",
    "distinguish between setosa and versicolor samples.\n",
    "\"\"\"\n",
    "features = np.asarray(list(zip(iris.loc[0:99, 'sepal_length'], iris.loc[0:99, 'petal_length'])))\n",
    "targets = np.where(iris.loc[0:99, 'species'] == 'Iris-setosa', 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now that we have our training data and targets, we will segment them into training and \n",
    "testing subsets using the sklearn train_test_split function, with 30% of the data being used\n",
    "to test.\n",
    "\"\"\"\n",
    "features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "    features, targets, stratify=targets, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First up to bat is the Perceptron. This is a very basic linear classification model\n",
    "and the first to draw inspiration from human neurons. The concept is very simple: Given \n",
    "a dataset with linearly separable features, it will compute a net sum using some weights\n",
    "that we will try and optimize. If the net sum is over some threshold, we have a positive output value,\n",
    "else we have a negative output value. This is based on the unit step function.\n",
    "\"\"\"\n",
    "class Perceptron(object):\n",
    "    \"\"\"\n",
    "    We initialize our model with a learning rate (affects how much we incremement our weights by,\n",
    "    a random_state that we will use when generating our initial weights, and the number of epochs our model\n",
    "    will run through during training. We give them default values for convenience.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, random_state=1, epochs=50):\n",
    "        self.learning_rate=0.01\n",
    "        self.random_state=random_state\n",
    "        self.epochs=epochs\n",
    "        \n",
    "    \"\"\"\n",
    "    This method will fit our model to the given training dataset.\n",
    "    --------------\n",
    "    Parameters\n",
    "    \n",
    "    features: np.ndarray: Each row should be an array of features for the training sample\n",
    "    targets: np.ndarray, array-like: Known class labels for each training sample.\n",
    "    \n",
    "    ----------------\n",
    "    Output\n",
    "    \n",
    "    A trained model with weights adjusted according to data set\n",
    "    \n",
    "    ---------------\n",
    "    \"\"\"\n",
    "    def fit(self, features, targets):\n",
    "        \n",
    "        ## In order for calculations later on to be smoother, we insert a column of 1's to the training dataset\n",
    "        features = np.insert(features, 0, 1, axis=1)\n",
    "        \n",
    "        ## Let us define some initial weights. We will draw from the normal distribution.\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=1.0, size=features[0].shape[0])\n",
    "        \n",
    "        ## We will run over the training samples {epochs} number of times\n",
    "        for _ in range(self.epochs):\n",
    "            for feature_vector, target in zip(features, targets):\n",
    "                output = self.predict(feature_vector)\n",
    "                error = target - output\n",
    "                \n",
    "                ## Update the weights based on error\n",
    "                self.w_ += self.learning_rate * error * feature_vector\n",
    "                \n",
    "        ## After running through specified epochs and adjusting weights, return trained model\n",
    "        return self\n",
    "        \n",
    "    ## Calculates the net sum for a given feature vector by multiplying weights with corresponding feature values\n",
    "    def net_sum(self, feature_vector):\n",
    "        return (self.w_.T.dot(feature_vector))\n",
    "    \n",
    "    ## Threshold function that predicts label based on net sum of features.\n",
    "    def predict(self, feature_vector):\n",
    "        net_sum = self.net_sum(feature_vector)\n",
    "        return np.where(net_sum >= 0.0, 1, -1)\n",
    "    \n",
    "    ## Will return the accuracy of model as a decimal\n",
    "    def test(self, test_features, test_targets):\n",
    "        outputs = []\n",
    "        test_features = np.insert(test_features, 0, 1, axis=1)\n",
    "        \n",
    "        for feature_vector in test_features:\n",
    "            outputs.append(self.predict(feature_vector))\n",
    "        \n",
    "        errors = 0\n",
    "        for i, output in enumerate(outputs):\n",
    "            if output != test_targets[i]:\n",
    "                errors += 1\n",
    "        \n",
    "        return 1.0 - (errors / len(test_targets))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron made predictions on test data with an accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here, we will train our perceptron, and test its performance\n",
    "\"\"\"\n",
    "ppn = Perceptron()\n",
    "ppn = ppn.fit(features_train, targets_train)\n",
    "ppn_accuracy = ppn.test(features_test, targets_test)\n",
    "\n",
    "print(f\"Perceptron made predictions on test data with an accuracy of {ppn_accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The next linear classification model we will implement is the Adaptive Linear Neuron, \n",
    "or ADALINE. This model will use gradient descent to optimize its cost function, ie. minimize\n",
    "the sum squared error. For it to be effective, we will first scale our test and train features \n",
    "using sklearn's StandardScaler\n",
    "\"\"\"\n",
    "sc = StandardScaler()\n",
    "sc = sc.fit(features_train)\n",
    "\n",
    "features_train_scaled = sc.transform(features_train)\n",
    "features_test_scaled = sc.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now, we implement our ADALINE model. It uses a simple linear activation function, phi(z) = z.\n",
    "Our threshold function remains more or less the same: T(x) = 1 if phi(z) >= 0.0, -1 otherwise.\n",
    "Or, T(x) = 1 if z >= 0.0, -1 otherwise. Weights are optimized using gradient descent.\n",
    "\"\"\"\n",
    "class Adaline(object):\n",
    "    def __init__(self, learning_rate=0.01, random_seed=1, epochs=50):\n",
    "        self.learning_rate = learning_rate,\n",
    "        self.random_seed = random_seed,\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def fit(self, features, targets):\n",
    "        \n",
    "        # Add 1 to each feature vector so that x_0 = 1 for multiplication with bias unit\n",
    "        features = np.insert(features, 0, 1, axis=1)\n",
    "        \n",
    "        ##Initialize weights using the normal distribution\n",
    "        rgen = np.random.RandomState(self.random_seed)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=1, size=features[0].shape[0])\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            errors = []\n",
    "            for feature_vector, target in zip(features, targets):\n",
    "                output = self.predict(feature_vector)\n",
    "                errors.append(target - output)\n",
    "            \n",
    "            ##Weight update based on all samples for gradient descent\n",
    "            update = self.learning_rate * (np.asarray(errors).dot(features))\n",
    "            self.w_ += update\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def net_sum(self, feature_vector):\n",
    "        return (self.w_.T.dot(feature_vector))\n",
    "    \n",
    "    def activate(self, net_sum):\n",
    "        return net_sum\n",
    "    \n",
    "    def predict(self, feature_vector):\n",
    "        net_sum = self.net_sum(feature_vector)\n",
    "        output = self.activate(net_sum)    ## This has no effect as activation function is linear in this case\n",
    "        \n",
    "        if output >= 0.0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def get_cost(self, errors):\n",
    "        errors = np.asarray(errors)\n",
    "        return (errors ** 2).sum()\n",
    "    \n",
    "    def test(self, test_features, test_targets):\n",
    "        outputs = []\n",
    "        test_features = np.insert(test_features, 0, 1, axis=1)\n",
    "        \n",
    "        for feature_vector in test_features:\n",
    "            outputs.append(self.predict(feature_vector))\n",
    "            \n",
    "        errors = 0\n",
    "        for i, prediction in enumerate(outputs):\n",
    "            if (prediction != test_targets[i]):\n",
    "                errors += 1\n",
    "                \n",
    "        return 1 - (errors / len(test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaline made predictions on test data with an accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We fit our Adaline implementation to the iris training data (after scaling)\n",
    "and test its effectiveness\n",
    "\"\"\"\n",
    "ad = Adaline()\n",
    "ad = ad.fit(features_train_scaled, targets_train)\n",
    "ad_accuracy = ad.test(features_test_scaled, targets_test)\n",
    "\n",
    "print(f\"Adaline made predictions on test data with an accuracy of {ad_accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Our final implementation of a linear learning model will be that of logistic regression\n",
    "which uses the sigmoid activation function. Optimization is achieved through gradient descent.\n",
    "Despite its name suggesting this is a Regression model, it is still a binary classifier.\n",
    "\"\"\"\n",
    "class LogisticRegression():\n",
    "    def __init__(self, learning_rate=0.01, random_seed=1, epochs=50):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_seed = random_seed\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def fit(self, features, targets):\n",
    "        ## Adding a column of 1's to represent x_0 and make calculations involving bias unit easier\n",
    "        features = np.insert(features, 0, 1, axis=1)\n",
    "        \n",
    "        ## Generating initial weights using the normal distribution\n",
    "        rgen = np.random.RandomState(self.random_seed)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=1, size=features[0].shape[0])\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            errors = []\n",
    "            for feature_vector, target in zip(features, targets):\n",
    "                output = self.predict(feature_vector)\n",
    "                errors.append(target - output)\n",
    "            \n",
    "            ## Update of weights very similar to method used in Adaline\n",
    "            update = self.learning_rate * (np.asarray(errors).dot(features))\n",
    "            self.w_ += update\n",
    "        \n",
    "        return self\n",
    "            \n",
    "    def net_sum(self, feature_vector):\n",
    "        return (self.w_.T.dot(feature_vector))\n",
    "    \n",
    "    def activate(self, net_sum):\n",
    "        return 1 / (1 + exp(-1 * net_sum))    ## Value given by sigmoid function\n",
    "    \n",
    "    def predict(self, feature_vector):\n",
    "        net_sum = self.net_sum(feature_vector)\n",
    "        output = self.activate(net_sum)\n",
    "        \n",
    "        if output >= 0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def test(self, features_test, targets_test):\n",
    "        features_test = np.insert(features_test, 0, 1, axis=1)\n",
    "        outputs = []\n",
    "        \n",
    "        for feature_vector in features_test:\n",
    "            outputs.append(self.predict(feature_vector))\n",
    "            \n",
    "        errors = 0\n",
    "        for i, prediction in enumerate(outputs):\n",
    "            if prediction != targets_test[i]:\n",
    "                errors += 1\n",
    "                \n",
    "        return 1 - (errors / len(targets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The activation function used by our Logistic Regression model has a \n",
    "range (0, 1), and so we need to adjust targets accordingly.\n",
    "\"\"\"\n",
    "lgr_targets_train = np.where(targets_train == -1, 0, 1)\n",
    "lgr_targets_test = np.where(targets_test == -1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model made predictions on test data with an accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training and testing our Logistic Regression model\n",
    "\"\"\"\n",
    "\n",
    "lgr = LogisticRegression()\n",
    "lgr = lgr.fit(features_train_scaled, lgr_targets_train)\n",
    "lgr_accuracy = lgr.test(features_test_scaled, lgr_targets_test)\n",
    "\n",
    "print(f\"Logistic Regression model made predictions on test data with an accuracy of {lgr_accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85</td>\n",
       "      <td>92</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>64</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86</td>\n",
       "      <td>54</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91</td>\n",
       "      <td>78</td>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87</td>\n",
       "      <td>70</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4    5  6\n",
       "0  85  92  45  27  31  0.0  1\n",
       "1  85  64  59  32  23  0.0  2\n",
       "2  86  54  33  16  54  0.0  2\n",
       "3  91  78  34  24  36  0.0  2\n",
       "4  87  70  12  28  10  0.0  2"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We have verified that our tests work for the iris dataset. As an additional measure\n",
    "we will test all of our models on another classic dataset: the BUPA liver disorders dataset.\n",
    "The models are not expected to generate accurate predictions as none of the classes are linearly\n",
    "separable from the others!\n",
    "\"\"\"\n",
    "data_url = 'ftp://ftp.ics.uci.edu/pub/machine-learning-databases/liver-disorders/bupa.data'\n",
    "bupa = pd.read_csv(data_url, header=None)\n",
    "bupa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separating features and targets.\n",
    "features = bupa.iloc[:, 0:6].values\n",
    "targets = np.where(bupa[6] == 2, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train, test split with 20% of the data being used to test our models\n",
    "features_train, features_test, targets_train, targets_test = train_test_split(features, targets, stratify=targets,\n",
    "                                                                             test_size = 0.2)\n",
    "\n",
    "## Special target data for logistic regression\n",
    "lgr_targets_train = np.where(targets_train == -1, 0, 1)\n",
    "lgr_targets_test = np.where(targets_test == -1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling features for faster gradient descent in Adaline and Logistic Regression\n",
    "sc = StandardScaler()\n",
    "sc = sc.fit(features_train)\n",
    "features_train_scaled = sc.transform(features_train)\n",
    "features_test_scaled = sc.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron made predictions on test data with an accuracy of 68.11594202898551%\n"
     ]
    }
   ],
   "source": [
    "## Testing our perceptron model with the new data\n",
    "ppn = Perceptron()\n",
    "ppn = ppn.fit(features_train_scaled, targets_train)\n",
    "ppn_accuracy = ppn.test(features_test_scaled, targets_test)\n",
    "\n",
    "print(f\"Perceptron made predictions on test data with an accuracy of {ppn_accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaline made predictions on test data with an accuracy of 55.072463768115945%\n"
     ]
    }
   ],
   "source": [
    "## Testing our adaline model with the new data\n",
    "ad = Adaline()\n",
    "ad = ad.fit(features_train_scaled, targets_train)\n",
    "ad_accuracy = ad.test(features_test_scaled, targets_test)\n",
    "\n",
    "print(f\"Adaline made predictions on test data with an accuracy of {ad_accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model made predictions on test data with an accuracy of 56.52173913043479%\n"
     ]
    }
   ],
   "source": [
    "## Testing our logical regression model with the new data\n",
    "lgr = LogisticRegression()\n",
    "lgr = lgr.fit(features_train_scaled, lgr_targets_train)\n",
    "lgr_accuracy = lgr.test(features_test_scaled, lgr_targets_test)\n",
    "\n",
    "print(f\"Logistic Regression model made predictions on test data with an accuracy of {lgr_accuracy * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
